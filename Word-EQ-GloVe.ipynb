{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-EQ-GloVe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/word-eq/blob/main/Word-EQ-GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCczNHAPTC_x"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q3Zjde3YFOE"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f_UivJ-MSTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0b638c-82aa-4891-85d4-c7d63cb81759"
      },
      "source": [
        "\"\"\"\n",
        "Download the Glove from drive.\n",
        "\"\"\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-\" -O glove.6B.zip && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 16:39:43--  https://docs.google.com/uc?export=download&confirm=bmAZ&id=1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.139, 142.250.141.113, 142.250.141.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e=download [following]\n",
            "--2021-12-10 16:39:43--  https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e=download\n",
            "Resolving doc-14-2o-docs.googleusercontent.com (doc-14-2o-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-14-2o-docs.googleusercontent.com (doc-14-2o-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=h4m5logf8rsb4&continue=https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e%3Ddownload&hash=pvv4a8jajlenrgidbh0kslmq61laimtt [following]\n",
            "--2021-12-10 16:39:44--  https://docs.google.com/nonceSigner?nonce=h4m5logf8rsb4&continue=https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e%3Ddownload&hash=pvv4a8jajlenrgidbh0kslmq61laimtt\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e=download&nonce=h4m5logf8rsb4&user=06236756351345617880Z&hash=jhg81b8c3eek5c92btkr0ai78in1k9d1 [following]\n",
            "--2021-12-10 16:39:44--  https://doc-14-2o-docs.googleusercontent.com/docs/securesc/bfhakek5oodf9897q1k2rqv1bovvq364/i3eqtmnlop7c0131015fotisuvfprsr4/1639154325000/04739181468756608208/06236756351345617880Z/1A-KM4ugWdi3rJ8lVKYZXdrAcfZQlkGj-?e=download&nonce=h4m5logf8rsb4&user=06236756351345617880Z&hash=jhg81b8c3eek5c92btkr0ai78in1k9d1\n",
            "Connecting to doc-14-2o-docs.googleusercontent.com (doc-14-2o-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  46.4MB/s    in 21s     \n",
            "\n",
            "2021-12-10 16:40:05 (39.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKkQqclxKBd1"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHesWpe8STzP",
        "outputId": "0401f04f-9399-4047-8264-97c7bf3243a7"
      },
      "source": [
        "#!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "\"\"\"\n",
        "Download the Glove 840B from drive.\n",
        "\"\"\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=18SddG0mv6bfRItzTUzRrlsuqLmtzS55r' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=18SddG0mv6bfRItzTUzRrlsuqLmtzS55r\" -O glove.840B.300d.zip && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 16:40:45--  https://docs.google.com/uc?export=download&confirm=hLgf&id=18SddG0mv6bfRItzTUzRrlsuqLmtzS55r\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.102, 142.250.141.101, 142.250.141.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e=download [following]\n",
            "--2021-12-10 16:40:45--  https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e=download\n",
            "Resolving doc-00-7c-docs.googleusercontent.com (doc-00-7c-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-00-7c-docs.googleusercontent.com (doc-00-7c-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=r4gu5b2v6fvno&continue=https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e%3Ddownload&hash=8h4smqcbtqcsdnspngffcdvcg6sf7fas [following]\n",
            "--2021-12-10 16:40:45--  https://docs.google.com/nonceSigner?nonce=r4gu5b2v6fvno&continue=https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e%3Ddownload&hash=8h4smqcbtqcsdnspngffcdvcg6sf7fas\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e=download&nonce=r4gu5b2v6fvno&user=04546961575664016603Z&hash=viel7ejnd7cf963cqovmnc5gf0a4qmj7 [following]\n",
            "--2021-12-10 16:40:45--  https://doc-00-7c-docs.googleusercontent.com/docs/securesc/gt3qjjlr6va9uj8v7nqs7jmc6tqt0h3d/vecf49q4u3bg8olbpu7ml5oi6r9e3sh1/1639154400000/04739181468756608208/04546961575664016603Z/18SddG0mv6bfRItzTUzRrlsuqLmtzS55r?e=download&nonce=r4gu5b2v6fvno&user=04546961575664016603Z&hash=viel7ejnd7cf963cqovmnc5gf0a4qmj7\n",
            "Connecting to doc-00-7c-docs.googleusercontent.com (doc-00-7c-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  67.0MB/s    in 37s     \n",
            "\n",
            "2021-12-10 16:41:23 (55.6 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3fwyP4jYVjT"
      },
      "source": [
        "!unzip -q glove.840B.300d.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uQabD0Vmxiu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMnclKbjNyru"
      },
      "source": [
        "# !cp \"/content/drive/MyDrive/audio-and-word-embeddings/eq_contributions.csv\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp85HcOXKNtt"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import csv\n",
        "from collections import Counter\n",
        "import random\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb5Dtj0213FH"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "508REITF0RAX"
      },
      "source": [
        "def read_annotation(filename):\n",
        "    events = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "        for row in spamreader:\n",
        "            events.append(row)\n",
        "    return events"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Download the SocialFX dataset for EQ settings and descriptors. \n",
        "\"\"\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF\" -O eq_contributions.csv && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "QAVjLElEOUK-",
        "outputId": "9ee6f832-307a-4e80-ba27-1341b8530557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 16:43:50--  https://docs.google.com/uc?export=download&confirm=&id=1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.113, 142.250.141.139, 142.250.141.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e=download [following]\n",
            "--2021-12-10 16:43:52--  https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e=download\n",
            "Resolving doc-00-bs-docs.googleusercontent.com (doc-00-bs-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-00-bs-docs.googleusercontent.com (doc-00-bs-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=e4tb0la52cr9q&continue=https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e%3Ddownload&hash=r4ve72tgtpju6l4tl6bjjpt83ass7vfl [following]\n",
            "--2021-12-10 16:43:52--  https://docs.google.com/nonceSigner?nonce=e4tb0la52cr9q&continue=https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e%3Ddownload&hash=r4ve72tgtpju6l4tl6bjjpt83ass7vfl\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e=download&nonce=e4tb0la52cr9q&user=13263907668047105798Z&hash=1fu3l3qmc1ajjbfmdnrbap212mqk9ju0 [following]\n",
            "--2021-12-10 16:43:52--  https://doc-00-bs-docs.googleusercontent.com/docs/securesc/3bkssvtrg742kl603ksgq529a6136kof/qdspvvs7dvi5p8eons4bdk30r6l18amm/1639154625000/04739181468756608208/13263907668047105798Z/1BrI0XPhzwDt7-_PZwtpsavtoPvRvjyKF?e=download&nonce=e4tb0la52cr9q&user=13263907668047105798Z&hash=1fu3l3qmc1ajjbfmdnrbap212mqk9ju0\n",
            "Connecting to doc-00-bs-docs.googleusercontent.com (doc-00-bs-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1294335 (1.2M) [text/csv]\n",
            "Saving to: ‘eq_contributions.csv’\n",
            "\n",
            "eq_contributions.cs 100%[===================>]   1.23M  6.49MB/s    in 0.2s    \n",
            "\n",
            "2021-12-10 16:43:52 (6.49 MB/s) - ‘eq_contributions.csv’ saved [1294335/1294335]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eE2lyca0aqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1ec54c-2d15-4459-aab5-f51a73700061"
      },
      "source": [
        "events = read_annotation(\"/content/eq_contributions.csv\")\n",
        "len(events)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1596"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxAsEoTzk8QD"
      },
      "source": [
        "words_list = [e[0].lower() for e in events[1:] if e[1] == \"English\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5vlTtsux-96",
        "outputId": "ee91e69b-b169-4dac-c3be-1883fe3a077e"
      },
      "source": [
        "len(words_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "918"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK_Lx-cYsZlG"
      },
      "source": [
        "hq_words = [\"bright\",\"brittle\",\"clear\",\"crisp\",\"harsh\",\"hollow\",\"sharp\",\"shrill\",\"tinny\",\"woody\",\"big\",\"boom\",\"boxy\",\"dark\",\"dull\",\"fat\",\"full\",\"muddy\",\"muffled\",\"punch\",\"smooth\",\"sweet\",\"warm\",\"flat\",\n",
        "            \"crunchy\",\"deep\",\"soothing\",\"clean\",\"airy\",\"cold\",\"metallic\",\"booming\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeBHe5PYrvS6",
        "outputId": "509774c9-e997-48a5-bbee-a39cf19d3bec"
      },
      "source": [
        "len(hq_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpXt8uTrk9_o",
        "outputId": "51e2e36c-948d-4b77-f736-81a17c04d681"
      },
      "source": [
        "words_list[0:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hot',\n",
              " 'wet',\n",
              " 'slow',\n",
              " 'saucy',\n",
              " 'heavy',\n",
              " 'shrill',\n",
              " 'warm',\n",
              " 'chill',\n",
              " 'sharp',\n",
              " 'corn']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDbe8cvgs9Zy"
      },
      "source": [
        "high_rated_words = list(set([e[0].lower() for e in events[1:] if e[1] == \"English\" and float(e[3]) > 0.7 and e[0].lower() not in hq_words]))\n",
        "high_rated_words.sort()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZ-dR7QZtF4"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri8vlWTOahc6",
        "outputId": "368ad275-b9b0-4d1d-ba8f-68c5ed6a9c4e"
      },
      "source": [
        "high_rated_words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aggressive',\n",
              " 'baseball',\n",
              " 'beautiful',\n",
              " 'biting',\n",
              " 'bold',\n",
              " 'brash',\n",
              " 'brass',\n",
              " 'brassy',\n",
              " 'breezy',\n",
              " 'calm',\n",
              " 'caring',\n",
              " 'cheerful',\n",
              " 'clarity',\n",
              " 'cooing',\n",
              " 'cool',\n",
              " 'cute',\n",
              " 'disgusting',\n",
              " 'down',\n",
              " 'edge',\n",
              " 'edgy',\n",
              " 'enchanting',\n",
              " 'energetic',\n",
              " 'energizing',\n",
              " 'excited',\n",
              " 'exciting',\n",
              " 'fancy',\n",
              " 'fluffy',\n",
              " 'forceful',\n",
              " 'frigid',\n",
              " 'funky',\n",
              " 'genius',\n",
              " 'gentle',\n",
              " 'good',\n",
              " 'gruff',\n",
              " 'happy',\n",
              " 'hard',\n",
              " 'heart-warming',\n",
              " 'heat',\n",
              " 'heavy',\n",
              " 'hot',\n",
              " 'huge',\n",
              " 'icy',\n",
              " 'jagged',\n",
              " 'large',\n",
              " 'light',\n",
              " 'loud',\n",
              " 'love',\n",
              " 'low',\n",
              " 'mellow',\n",
              " 'mournful',\n",
              " 'noisy',\n",
              " 'passionate',\n",
              " 'peace',\n",
              " 'peaceful',\n",
              " 'pleasing',\n",
              " 'pleasurable',\n",
              " 'plodding',\n",
              " 'poor',\n",
              " 'punchy',\n",
              " 'quiet',\n",
              " 'radiant',\n",
              " 'relaxing',\n",
              " 'reserved',\n",
              " 'rich',\n",
              " 'romantic',\n",
              " 'rousing',\n",
              " 'rumble',\n",
              " 'serene',\n",
              " 'sloppy',\n",
              " 'slow',\n",
              " 'soft',\n",
              " 'solemn',\n",
              " 'splash',\n",
              " 'squeaking',\n",
              " 'strong',\n",
              " 'taco',\n",
              " 'techno',\n",
              " 'tense',\n",
              " 'throbbing',\n",
              " 'thumpy',\n",
              " 'thunderous',\n",
              " 'twangy',\n",
              " 'velvety',\n",
              " 'whispered',\n",
              " 'whispering',\n",
              " 'wistful']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJnDzsCuN63",
        "outputId": "3a18b9a2-8d66-46df-9de8-2eb202257a07"
      },
      "source": [
        "len(high_rated_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqA0HSlnta2F"
      },
      "source": [
        "# 5, 19, 71\n",
        "# Using seed 137 for shuffling into folds\n",
        "random.seed(137)\n",
        "random.shuffle(hq_words)\n",
        "random.shuffle(high_rated_words)\n",
        "#test_word_set = hq_words[0:10] + high_rated_words[0:30]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV6DaeZJsCsz"
      },
      "source": [
        "test_word_set_1 = hq_words[0:9] + high_rated_words[0:22]\n",
        "test_word_set_2 = hq_words[9:18] + high_rated_words[22:44]\n",
        "test_word_set_3 = hq_words[18:27] + high_rated_words[44:66]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNc_qCz6swHa"
      },
      "source": [
        "test_word_set_4 = hq_words[27:] + hq_words[0:4] + high_rated_words[66:] + high_rated_words[0:2]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-_n-Vu50Dpt",
        "outputId": "44b0dce0-a5a9-4124-88fa-f0ec46d57526"
      },
      "source": [
        "test_word_set_4"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sharp',\n",
              " 'big',\n",
              " 'dark',\n",
              " 'hollow',\n",
              " 'harsh',\n",
              " 'smooth',\n",
              " 'muffled',\n",
              " 'crisp',\n",
              " 'punch',\n",
              " 'mournful',\n",
              " 'clarity',\n",
              " 'genius',\n",
              " 'bold',\n",
              " 'twangy',\n",
              " 'soft',\n",
              " 'splash',\n",
              " 'slow',\n",
              " 'wistful',\n",
              " 'brash',\n",
              " 'fancy',\n",
              " 'cute',\n",
              " 'rousing',\n",
              " 'loud',\n",
              " 'breezy',\n",
              " 'large',\n",
              " 'passionate',\n",
              " 'baseball',\n",
              " 'huge',\n",
              " 'icy',\n",
              " 'brassy',\n",
              " 'caring']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_cOlQAqRixG"
      },
      "source": [
        "cc = dict(Counter(words_list))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_49I4ny0S5mD"
      },
      "source": [
        "words_set = list(cc.keys())\n",
        "words_set.sort()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrCAZnfO0LFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4267ba91-22b0-4ed6-80e2-b7d0b445ad8c"
      },
      "source": [
        "len(words_set)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "388"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAVFcKCGfHXZ",
        "outputId": "fa4577d4-f2b6-41f1-af9b-d7241f054256"
      },
      "source": [
        "cc"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aggravating': 2,\n",
              " 'aggressive': 4,\n",
              " 'agitated': 1,\n",
              " 'aidan': 1,\n",
              " 'airy': 3,\n",
              " 'alluring': 1,\n",
              " 'anger': 1,\n",
              " 'angry': 2,\n",
              " 'annoying': 1,\n",
              " 'anticipation': 1,\n",
              " 'apple': 1,\n",
              " 'archaic': 1,\n",
              " 'awesome': 2,\n",
              " 'awkward': 1,\n",
              " 'bag': 1,\n",
              " 'balanced': 1,\n",
              " 'bang': 1,\n",
              " 'baseball': 1,\n",
              " 'bassy': 1,\n",
              " 'bear': 1,\n",
              " 'beautiful': 10,\n",
              " 'big': 1,\n",
              " 'biting': 1,\n",
              " 'blaring': 1,\n",
              " 'blast': 1,\n",
              " 'boisterous': 1,\n",
              " 'bold': 2,\n",
              " 'bonito': 1,\n",
              " 'boom': 1,\n",
              " 'booming': 1,\n",
              " 'bored': 1,\n",
              " 'bouncy': 1,\n",
              " 'boxy': 1,\n",
              " 'brash': 1,\n",
              " 'brass': 1,\n",
              " 'brassy': 2,\n",
              " 'brave': 1,\n",
              " 'breezy': 1,\n",
              " 'bright': 19,\n",
              " 'brillante': 1,\n",
              " 'brittle': 1,\n",
              " 'buzz': 2,\n",
              " 'calm': 13,\n",
              " 'car': 1,\n",
              " 'carefree': 1,\n",
              " 'caring': 1,\n",
              " 'cat': 2,\n",
              " 'caustic': 1,\n",
              " 'cerebral': 1,\n",
              " 'charming': 1,\n",
              " 'cheerful': 3,\n",
              " 'cheery': 1,\n",
              " 'children': 1,\n",
              " 'chill': 1,\n",
              " 'chilly': 1,\n",
              " 'choppy': 1,\n",
              " 'chunky': 2,\n",
              " 'clarity': 1,\n",
              " 'clash': 1,\n",
              " 'classic': 2,\n",
              " 'classy': 1,\n",
              " 'clean': 4,\n",
              " 'clear': 12,\n",
              " 'coarse': 1,\n",
              " 'cold': 34,\n",
              " 'computer': 1,\n",
              " 'confident': 1,\n",
              " 'congested': 1,\n",
              " 'cooing': 1,\n",
              " 'cool': 14,\n",
              " 'corn': 1,\n",
              " 'cosy': 1,\n",
              " 'cow': 1,\n",
              " 'crackling': 1,\n",
              " 'crash': 1,\n",
              " 'creepy': 1,\n",
              " 'cricket': 1,\n",
              " 'crisp': 8,\n",
              " 'crunch': 1,\n",
              " 'crunchy': 5,\n",
              " 'cute': 1,\n",
              " 'damp': 1,\n",
              " 'dangerous': 1,\n",
              " 'dark': 8,\n",
              " 'darkness': 1,\n",
              " 'deafening': 1,\n",
              " 'deep': 6,\n",
              " 'delicious': 1,\n",
              " 'delightful': 1,\n",
              " 'depressed': 1,\n",
              " 'discombobulated': 1,\n",
              " 'disgusted': 1,\n",
              " 'disgusting': 1,\n",
              " 'distant': 2,\n",
              " 'distracting': 1,\n",
              " 'dog': 1,\n",
              " 'down': 1,\n",
              " 'dramatic': 1,\n",
              " 'dreamy': 1,\n",
              " 'drive': 1,\n",
              " 'drums': 1,\n",
              " 'dry': 2,\n",
              " 'dull': 3,\n",
              " 'dynamic': 1,\n",
              " 'eager': 1,\n",
              " 'easy': 1,\n",
              " 'ecstasy': 1,\n",
              " 'edge': 1,\n",
              " 'edgy': 1,\n",
              " 'elegant': 3,\n",
              " 'emotional': 3,\n",
              " 'enchanting': 3,\n",
              " 'energetic': 5,\n",
              " 'energized': 1,\n",
              " 'energizing': 1,\n",
              " 'enthusiastic': 1,\n",
              " 'enticing': 1,\n",
              " 'epic': 1,\n",
              " 'euphoric': 1,\n",
              " 'even': 1,\n",
              " 'evil': 1,\n",
              " 'excited': 3,\n",
              " 'excitement': 1,\n",
              " 'exciting': 3,\n",
              " 'famous': 1,\n",
              " 'fancy': 1,\n",
              " 'fast': 5,\n",
              " 'fast-paced': 1,\n",
              " 'fat': 2,\n",
              " 'favourable': 1,\n",
              " 'fiery': 1,\n",
              " 'fire': 1,\n",
              " 'flat': 3,\n",
              " 'fleeting': 1,\n",
              " 'floating': 1,\n",
              " 'flow': 1,\n",
              " 'fluffy': 1,\n",
              " 'foggy': 1,\n",
              " 'food': 2,\n",
              " 'fool': 1,\n",
              " 'forceful': 1,\n",
              " 'forgetful': 1,\n",
              " 'frantic': 1,\n",
              " 'fresh': 2,\n",
              " 'frightening': 1,\n",
              " 'frigid': 1,\n",
              " 'frosty': 1,\n",
              " 'frustration': 1,\n",
              " 'full': 2,\n",
              " 'fun': 4,\n",
              " 'funky': 4,\n",
              " 'fuzzy': 1,\n",
              " 'game dog': 1,\n",
              " 'genius': 1,\n",
              " 'gentle': 8,\n",
              " 'god': 1,\n",
              " 'good': 3,\n",
              " 'good morning': 1,\n",
              " 'grace': 1,\n",
              " 'graceful': 2,\n",
              " 'gradual': 1,\n",
              " 'grainy': 2,\n",
              " 'grand': 2,\n",
              " 'grandiose': 1,\n",
              " 'grating': 1,\n",
              " 'greasy': 1,\n",
              " 'great': 1,\n",
              " 'grieving': 1,\n",
              " 'groovy': 1,\n",
              " 'gruff': 1,\n",
              " 'guitar': 1,\n",
              " 'happy': 22,\n",
              " 'hard': 10,\n",
              " 'hardness': 1,\n",
              " 'harmonious': 1,\n",
              " 'harmony': 1,\n",
              " 'harsh': 16,\n",
              " 'hate': 3,\n",
              " 'heart-warming': 1,\n",
              " 'heat': 1,\n",
              " 'heavy': 15,\n",
              " 'hello': 3,\n",
              " 'helpful': 1,\n",
              " 'hero': 1,\n",
              " 'high': 2,\n",
              " 'high-pitched': 2,\n",
              " 'hollow': 3,\n",
              " 'honest': 1,\n",
              " 'hot': 8,\n",
              " 'huge': 1,\n",
              " 'humble': 1,\n",
              " 'humid': 1,\n",
              " 'icy': 1,\n",
              " 'impatient': 1,\n",
              " 'important': 1,\n",
              " 'infernal': 1,\n",
              " 'inspirational': 1,\n",
              " 'intelligent': 1,\n",
              " 'invigorate': 1,\n",
              " 'jagged': 1,\n",
              " 'jarring': 3,\n",
              " 'jazzy': 1,\n",
              " 'joy': 1,\n",
              " 'joyful': 1,\n",
              " 'joyous': 2,\n",
              " 'jumpy': 1,\n",
              " 'kind': 1,\n",
              " 'large': 2,\n",
              " 'lazy': 1,\n",
              " 'light': 6,\n",
              " 'lightening': 1,\n",
              " 'lindo': 1,\n",
              " 'little': 1,\n",
              " 'lively': 1,\n",
              " 'loud': 26,\n",
              " 'love': 4,\n",
              " 'low': 2,\n",
              " 'man': 1,\n",
              " 'martial': 1,\n",
              " 'marvellous': 1,\n",
              " 'mean': 1,\n",
              " 'meaty': 1,\n",
              " 'melancholy': 1,\n",
              " 'mellow': 8,\n",
              " 'melodic': 4,\n",
              " 'melodious': 3,\n",
              " 'melody': 1,\n",
              " 'metal': 1,\n",
              " 'metallic': 2,\n",
              " 'mischievous': 2,\n",
              " 'moan': 1,\n",
              " 'moody': 1,\n",
              " 'morning': 2,\n",
              " 'mother': 1,\n",
              " 'motive': 1,\n",
              " 'mournful': 1,\n",
              " 'muddled': 1,\n",
              " 'muddy': 2,\n",
              " 'muffled': 6,\n",
              " 'muscle': 1,\n",
              " 'music': 1,\n",
              " 'mysterious': 1,\n",
              " 'nasty': 1,\n",
              " 'nice': 3,\n",
              " 'noise': 1,\n",
              " 'noisy': 6,\n",
              " 'obnoxious': 1,\n",
              " 'ominous': 1,\n",
              " 'optimistic': 1,\n",
              " 'oxygen': 1,\n",
              " 'painful': 1,\n",
              " 'passionate': 1,\n",
              " 'peace': 1,\n",
              " 'peaceful': 7,\n",
              " 'pessimistic': 1,\n",
              " 'placid': 1,\n",
              " 'playful': 1,\n",
              " 'pleasant': 6,\n",
              " 'please': 1,\n",
              " 'pleasing': 2,\n",
              " 'pleasurable': 1,\n",
              " 'plodding': 1,\n",
              " 'poor': 1,\n",
              " 'pop': 1,\n",
              " 'power': 2,\n",
              " 'power-puff': 1,\n",
              " 'powerful': 2,\n",
              " 'pretty': 2,\n",
              " 'prickly': 1,\n",
              " 'pumping': 1,\n",
              " 'punch': 1,\n",
              " 'punchy': 2,\n",
              " 'pure': 1,\n",
              " 'quick': 1,\n",
              " 'quiet': 5,\n",
              " 'radiant': 1,\n",
              " 'radical': 1,\n",
              " 'rage': 1,\n",
              " 'rain': 1,\n",
              " 'ralph': 1,\n",
              " 'raspy': 3,\n",
              " 'relaxed': 1,\n",
              " 'relaxing': 4,\n",
              " 'reserved': 1,\n",
              " 'rhythmic': 1,\n",
              " 'rich': 4,\n",
              " 'rigid': 1,\n",
              " 'riotous': 1,\n",
              " 'roar': 2,\n",
              " 'rock': 4,\n",
              " 'roll': 1,\n",
              " 'romantic': 4,\n",
              " 'rough': 1,\n",
              " 'rousing': 1,\n",
              " 'rumble': 2,\n",
              " 'saccharine': 1,\n",
              " 'sad': 9,\n",
              " 'sadness': 1,\n",
              " 'safe': 1,\n",
              " 'saucy': 1,\n",
              " 'scabrous': 1,\n",
              " 'scared': 1,\n",
              " 'scary': 5,\n",
              " 'scratchy': 1,\n",
              " 'screeching': 1,\n",
              " 'sea': 1,\n",
              " 'sensual': 1,\n",
              " 'serene': 3,\n",
              " 'serious': 1,\n",
              " 'severe': 1,\n",
              " 'sharp': 8,\n",
              " 'shock': 1,\n",
              " 'shrill': 4,\n",
              " 'sincere': 1,\n",
              " 'sinister': 1,\n",
              " 'sloppy': 1,\n",
              " 'slow': 2,\n",
              " 'sluggish': 1,\n",
              " 'small': 1,\n",
              " 'smart': 1,\n",
              " 'smelly': 1,\n",
              " 'smooth': 14,\n",
              " 'snowball': 1,\n",
              " 'soft': 29,\n",
              " 'solemn': 1,\n",
              " 'solid': 1,\n",
              " 'somber': 1,\n",
              " 'soothing': 17,\n",
              " 'sound': 1,\n",
              " 'spacey': 1,\n",
              " 'spine-tingling': 1,\n",
              " 'splash': 1,\n",
              " 'splendid': 1,\n",
              " 'spooky': 1,\n",
              " 'sprightly': 1,\n",
              " 'squeaking': 1,\n",
              " 'steep': 1,\n",
              " 'stormy': 1,\n",
              " 'strong': 2,\n",
              " 'subtle': 1,\n",
              " 'sunshine': 1,\n",
              " 'super': 2,\n",
              " 'surprised': 1,\n",
              " 'sweet': 5,\n",
              " 'table tennis': 1,\n",
              " 'taco': 1,\n",
              " 'techno': 1,\n",
              " 'tender': 1,\n",
              " 'tense': 2,\n",
              " 'thick': 1,\n",
              " 'throbbing': 2,\n",
              " 'thumpy': 1,\n",
              " 'thunderous': 1,\n",
              " 'tiger': 1,\n",
              " 'tight': 1,\n",
              " 'tinny': 8,\n",
              " 'toasty': 1,\n",
              " 'tough': 2,\n",
              " 'tragic': 1,\n",
              " 'tranquil': 1,\n",
              " 'tremble': 1,\n",
              " 'trimble': 1,\n",
              " 'tropical': 1,\n",
              " 'twangy': 1,\n",
              " 'upbeat': 3,\n",
              " 'uplifting': 4,\n",
              " 'vague': 1,\n",
              " 'velvety': 1,\n",
              " 'vibrant': 1,\n",
              " 'vicious': 1,\n",
              " 'victorious': 1,\n",
              " 'vintage': 1,\n",
              " 'violent': 1,\n",
              " 'warm': 64,\n",
              " 'watery': 1,\n",
              " 'welcome': 2,\n",
              " 'wet': 2,\n",
              " 'whispered': 1,\n",
              " 'whispering': 1,\n",
              " 'wild': 2,\n",
              " 'winter': 1,\n",
              " 'wise': 1,\n",
              " 'wistful': 2,\n",
              " 'woody': 1,\n",
              " 'workout': 1,\n",
              " 'worried': 1,\n",
              " 'wow': 1,\n",
              " 'youthful': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNCG9A97fUME"
      },
      "source": [
        "# Seeds are changed below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nLhPluEUmYK"
      },
      "source": [
        "train_word_set = [w for w in words_set if w not in test_word_set_1]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz9u2k8PZb6s"
      },
      "source": [
        "train_events = [e for e in events[1:] if e[1] == \"English\" and e[0].lower() in train_word_set]\n",
        "#val_events = [e for e in events[1:] if e[1] == \"English\" and e[0] in val_word_set]\n",
        "test_events = [e for e in events[1:] if e[1] == \"English\" and e[0].lower() in test_word_set_1 and float(e[3]) > 0.7]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YAdkkh-4WhU"
      },
      "source": [
        "words_set = list(cc.keys())\n",
        "words_set.sort()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LJjEjsVgeK9"
      },
      "source": [
        "train_samples = [e[0].lower() for e in train_events]\n",
        "train_labels = [e[4:] for e in train_events]\n",
        "\n",
        "# val_samples = [e[0] for e in val_events]\n",
        "# val_labels = [e[4:] for e in val_events]\n",
        "\n",
        "test_samples = [e[0].lower() for e in test_events]\n",
        "test_labels = [e[4:] for e in test_events]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Nj4cJUoZb3"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E5lM9C6Z1Jn",
        "outputId": "f173fbb6-74c2-47f5-8d5b-a5dab79f67a8"
      },
      "source": [
        "print(len(train_events))\n",
        "# print(len(val_events))\n",
        "print(len(test_events))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "792\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6f-QtHi1Tt5"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNFlVrFiVeO3",
        "outputId": "741b70f1-07c9-4439-ee21-191e2a5cab1f"
      },
      "source": [
        "\"warm\" in train_word_set"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ews3dTDR3CU",
        "outputId": "2a5b6117-02d2-4c2f-f6c6-f447fdfc282b"
      },
      "source": [
        "cc['bright']"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "oKxZWbxon72V",
        "outputId": "5900b6be-6e5b-43c4-fde4-a34676d46dd9"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/glove.840B.300d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d1ae2711245d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waoFY4JylNxw"
      },
      "source": [
        "vec2 = TextVectorization(max_tokens=400010, output_sequence_length=1)\n",
        "eq_ds = tf.data.Dataset.from_tensor_slices(words_set).batch(128)\n",
        "vec2.adapt(eq_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cVQfr18h2qp"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_C0j0Koh4KS"
      },
      "source": [
        "# download and import the large english model.\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD3xmjmrh4M1"
      },
      "source": [
        "nlp = en_core_web_lg.load()\n",
        "vec2 = TextVectorization(output_sequence_length=1)\n",
        "eq_ds = tf.data.Dataset.from_tensor_slices(words_set).batch(128)\n",
        "vec2.adapt(eq_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUChuPypLMS"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgczfa4fmuk_"
      },
      "source": [
        "Vectorizer = TextVectorization()\n",
        "\n",
        "#load the data \n",
        "text = pd.read_csv('https://github.com/Violet-Spiral/assessing-childrens-writing/raw/main/data/samples_no_title.csv').dropna()\n",
        "\n",
        "#fit the vectorizer on the text and extract the corpus vocabulary\n",
        "Vectorizer.adapt(text.Text.to_numpy())\n",
        "vocab = Vectorizer.get_vocabulary()\n",
        "\n",
        "#generate the embedding matrix\n",
        "num_tokens = len(vocab)\n",
        "embedding_dim = len(nlp('The').vector)\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_matrix[i] = nlp(str(word)).vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRbcQ_zheYkw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw5gyEXGh4Ps"
      },
      "source": [
        "vec2 = TextVectorization(max_tokens=400010, output_sequence_length=1)\n",
        "eq_ds = tf.data.Dataset.from_tensor_slices(list(embeddings_index.keys())).batch(128)\n",
        "vec2.adapt(eq_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMYhRY1h4Rr"
      },
      "source": [
        "voc2 = vec2.get_vocabulary()\n",
        "word_index_2 = dict(zip(voc2, range(len(voc2))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-1dqkHZaP-O"
      },
      "source": [
        "len(voc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDHtOlHvlkJk"
      },
      "source": [
        "word_index_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyb27S41jj8L"
      },
      "source": [
        "#generate the embedding matrix for spacy\n",
        "num_tokens = len(voc2)\n",
        "embedding_dim = len(nlp('The').vector)\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(word_index_2):\n",
        "    embedding_matrix[i] = nlp(str(word)).vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apkf_iSm9KJg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBjcfARr9KTX"
      },
      "source": [
        "em,hit = read_annotation_2(\"/content/dict2vec300/dict2vec-300d.vec\", \"warm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEYe7vFA9pRJ"
      },
      "source": [
        "em"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4XZZFf2vI6"
      },
      "source": [
        "def read_annotation_2(filename, word):\n",
        "    events = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "        for row in spamreader:\n",
        "            if row[0] == word:\n",
        "              return row,True\n",
        "        return -1,False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhhRcR4O5PUa"
      },
      "source": [
        "# Embedding matrix for dict2vec\n",
        "#generate the embedding matrix\n",
        "\n",
        "voc2 = vec2.get_vocabulary()\n",
        "word_index_2 = dict(zip(voc2, range(len(voc2))))\n",
        "\n",
        "num_tokens = len(voc2)\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(word_index_2):\n",
        "    if word == '' or word == '[UNK]':\n",
        "      embedding_matrix[i] = np.zeros(embedding_dim,)\n",
        "      print(\"Missed: \" + word)\n",
        "      continue\n",
        "    em,hit = read_annotation_2(\"/content/dict2vec-300d.vec\", word)\n",
        "    if not hit:\n",
        "      embedding_matrix[i] = np.zeros(embedding_dim,)\n",
        "      print(\"Missed: \" + word)\n",
        "\n",
        "    else:\n",
        "      em_2 = np.array(em[1:-1]).astype(float)\n",
        "      embedding_matrix[i] = em_2\n",
        "      print(\"Added: \" + word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvvjG4xAx-fe"
      },
      "source": [
        "max(nlp('keyboard').vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI-hDAcEnU9F"
      },
      "source": [
        "type(word_index_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdwSe3oAj_zk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlIhbMmj_5H"
      },
      "source": [
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index_2.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        print(word)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej-PcL4nl2Pb"
      },
      "source": [
        "vec2.get_vocabulary()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMSCXE9Kl5jY"
      },
      "source": [
        "output = vec2([[\"loud\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xdA8xaVmLGI"
      },
      "source": [
        "voc2 = vec2.get_vocabulary()\n",
        "word_index_2 = dict(zip(voc2, range(len(voc2))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trCZH29Vx4Y1"
      },
      "source": [
        "voc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fgTmkSOoJ6k"
      },
      "source": [
        "len(voc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTuqefWen4q2"
      },
      "source": [
        "test = [\"warm\", \"cold\", \"soft\"]\n",
        "[word_index_2[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdk6dumSNxSR"
      },
      "source": [
        "from scipy.spatial.distance import cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LziqLbf9OW8t"
      },
      "source": [
        "plt.plot(embeddings_index['hot'])\n",
        "plt.plot(embeddings_index['cold'])\n",
        "plt.plot(embeddings_index['ocean'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_ir0xc7Njpf"
      },
      "source": [
        "cosine(embeddings_index['fire'], embeddings_index['burn'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDKRMotXr80b"
      },
      "source": [
        "embedding_matrix = np.random.rand(num_tokens, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HelixDJgoXdx"
      },
      "source": [
        "num_tokens = len(voc2) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix for GloVe\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index_2.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        print(word)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmgxDL3Jf9Uq"
      },
      "source": [
        "# x_train = vec2(np.array([[s] for s in train_samples])).numpy()\n",
        "# x_val = vec2(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "# y_train = np.array(train_labels, dtype='float')\n",
        "# y_val = np.array(val_labels, dtype='float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpgK198s6j_T"
      },
      "source": [
        "# Data is normalised below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWsUf0Hqkxg"
      },
      "source": [
        "x_train = vec2(np.array([[s] for s in train_samples])).numpy()\n",
        "#x_val = vec2(np.array([[s] for s in val_samples])).numpy()\n",
        "x_test = vec2(np.array([[s] for s in test_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels, dtype='float')[:, :]\n",
        "#y_val = np.array(val_labels, dtype='float')[:, :]\n",
        "y_test = np.array(test_labels, dtype='float')[:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHALibFTacB"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdAAWj4Es3cc"
      },
      "source": [
        "mmin = np.ones((40,)) * -4\n",
        "mmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8R6da-mtCXf"
      },
      "source": [
        "mmax = np.ones((40,)) * 4\n",
        "mmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBKyn7M3o9c0"
      },
      "source": [
        "# y_combined = np.concatenate((y_train, y_val, y_test), axis = 0)\n",
        "# mmax = y_combined.max(axis=0)\n",
        "# mmin = y_combined.min(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS4lq5mYssFU"
      },
      "source": [
        "# mmax = np.ones()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-CFFYQ1SPEy"
      },
      "source": [
        "y_train = (y_train - mmin) / (mmax - mmin)\n",
        "#y_val = (y_val - mmin) / (mmax - mmin)\n",
        "y_test = (y_test - mmin) / (mmax - mmin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_A09Rw8FLMi"
      },
      "source": [
        "print(np.max(y_train))\n",
        "#print(np.max(y_val))\n",
        "print(np.max(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn5fXIXuWyZU"
      },
      "source": [
        "num_tokens = 392\n",
        "embedding_dim = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USlm2MAK4e1w"
      },
      "source": [
        "initial_learning_rate = 0.1\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyuzqSkNo3oR"
      },
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    # embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    # trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Jeq50q1zzL"
      },
      "source": [
        "num_tokens, embedding_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OHVtIUXpGoF"
      },
      "source": [
        "def mean_abs_error_0(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 0] - y_true[:, 0])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_1(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 1] - y_true[:, 1])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_2(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 2] - y_true[:, 2])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_3(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 3] - y_true[:, 3])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_4(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 4] - y_true[:, 4])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_5(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 5] - y_true[:, 5])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_6(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 6] - y_true[:, 6])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_7(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 7] - y_true[:, 7])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_8(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 8] - y_true[:, 8])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_9(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 9] - y_true[:, 9])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_10(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 10] - y_true[:, 10])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_11(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 11] - y_true[:, 11])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_12(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 12] - y_true[:, 12])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_13(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 13] - y_true[:, 13])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_14(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 14] - y_true[:, 14])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_15(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 15] - y_true[:, 15])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_16(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 16] - y_true[:, 16])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_17(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 17] - y_true[:, 17])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_18(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 18] - y_true[:, 18])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_19(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 19] - y_true[:, 19])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_20(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 20] - y_true[:, 20])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_21(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 21] - y_true[:, 21])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_22(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 22] - y_true[:, 22])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_23(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 23] - y_true[:, 23])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_24(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 24] - y_true[:, 24])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_25(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 25] - y_true[:, 25])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_26(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 26] - y_true[:, 26])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_27(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 27] - y_true[:, 27])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_28(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 28] - y_true[:, 28])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_29(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 29] - y_true[:, 29])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_30(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 30] - y_true[:, 30])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_31(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 31] - y_true[:, 31])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_32(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 32] - y_true[:, 32])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_33(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 33] - y_true[:, 33])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_34(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 34] - y_true[:, 34])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_35(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 35] - y_true[:, 35])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_36(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 36] - y_true[:, 36])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_37(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 37] - y_true[:, 37])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_38(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 38] - y_true[:, 38])\n",
        "  return tf.reduce_mean(diff)\n",
        "\n",
        "def mean_abs_error_39(y_true, y_pred):\n",
        "  diff = tf.abs(y_pred[:, 39] - y_true[:, 39])\n",
        "  return tf.reduce_mean(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Civ4xi9nhNTM"
      },
      "source": [
        "\"\"\"\n",
        "Currently the best performance is when having a 4-unit layer in between with LeakyRelu activations, SGD optimizer with LR of 0.001.\n",
        "In one of the random restarts, it reached ~ 0.93.\n",
        "It was better with augmentation of GaussianNoise(0.3) and above learning rate schedule [0.01, 5000, 0.96].\n",
        "\"\"\"\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(1,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = embedded_sequences\n",
        "# x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Lambda(lambda xx: tf.squeeze(xx, axis=-2))(x)\n",
        "x = layers.Dense(300, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "x = layers.Dense(200, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "x = layers.Dense(100, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "x = layers.Dense(80, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "x = layers.Dense(60, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "# x = layers.Dense(40, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.05)(x)\n",
        "preds = layers.Dense(40, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.MeanAbsoluteError(), optimizer=keras.optimizers.SGD(learning_rate=lr_schedule), \n",
        "    metrics=[\n",
        "            #  mean_abs_error_0,\n",
        "            #  mean_abs_error_1,\n",
        "            #  mean_abs_error_2,\n",
        "            #  mean_abs_error_3,\n",
        "            #  mean_abs_error_4,\n",
        "            #  mean_abs_error_5,\n",
        "            #  mean_abs_error_6,\n",
        "            #  mean_abs_error_7,\n",
        "            #  mean_abs_error_8,\n",
        "            #  mean_abs_error_9,\n",
        "            #  mean_abs_error_10,\n",
        "            #  mean_abs_error_11,\n",
        "            #  mean_abs_error_12,\n",
        "            #  mean_abs_error_13,\n",
        "            #  mean_abs_error_14,\n",
        "            #  mean_abs_error_15,\n",
        "            #  mean_abs_error_16,\n",
        "            #  mean_abs_error_17,\n",
        "            #  mean_abs_error_18,\n",
        "            #  mean_abs_error_19,\n",
        "            #  mean_abs_error_20,\n",
        "            #  mean_abs_error_21,\n",
        "            #  mean_abs_error_22,\n",
        "            #  mean_abs_error_23,\n",
        "            #  mean_abs_error_24,\n",
        "            #  mean_abs_error_25,\n",
        "            #  mean_abs_error_26,\n",
        "            #  mean_abs_error_27,\n",
        "            #  mean_abs_error_28,\n",
        "            #  mean_abs_error_29,\n",
        "            #  mean_abs_error_30,   \n",
        "            #  mean_abs_error_31,\n",
        "            #  mean_abs_error_32,\n",
        "            #  mean_abs_error_33,\n",
        "            #  mean_abs_error_34,\n",
        "            #  mean_abs_error_35,\n",
        "            #  mean_abs_error_36,\n",
        "            #  mean_abs_error_37,\n",
        "            #  mean_abs_error_38,\n",
        "            #  mean_abs_error_39,                                               \n",
        "            #  #keras.metrics.MeanAbsoluteError(), \n",
        "            #  tf.keras.metrics.MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\", dtype=None), \n",
        "#             keras.metrics.AUC(multi_label=True),\n",
        "tf.keras.losses.MeanAbsolutePercentageError(\n",
        "    reduction=\"auto\", name=\"mean_absolute_percentage_error\")\n",
        "]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-8Gf8mdh6o5"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=500, restore_best_weights=True)\n",
        "history = model.fit(x_train, y_train, epochs=20000, validation_data=(x_test, y_test), verbose=2, callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib8KUCuO2hwG"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/trained models on 19-11-21/glove-6B-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSeLZrKcl25s"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/trained models on 19-11-21/random-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDzrdZNF1CAX"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XywR0V7fLws6"
      },
      "source": [
        "#Spacey fold-1\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58n0RGIL-xoo"
      },
      "source": [
        "#Spacey fold-2\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxRwox24Bgpp"
      },
      "source": [
        "#Spacey fold-3\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbrwaElCX4nv"
      },
      "source": [
        "model.save_weights(\"spacey-random-2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nFiB-mSxOQw"
      },
      "source": [
        "# Random model evaluation with trainable=True\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMPrgsDA_MHp"
      },
      "source": [
        "# Random model evaluation with trainable=True \n",
        "# fold-2\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEK7GGlWBrSy"
      },
      "source": [
        "# Random model evaluation with trainable=True \n",
        "# fold-3\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZO-dHho9NoA"
      },
      "source": [
        "# Evaluate Glove embeddings\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZuC65m4bBZJ"
      },
      "source": [
        "# Evaluate Glove 840B embeddings\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_thNN49ieRkF"
      },
      "source": [
        "# Evaluate Glove 840B embeddings\n",
        "# Fold-2\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOzqbLwTCmqc"
      },
      "source": [
        "# Evaluate dict2vec embeddings\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoiabEje1Dcg"
      },
      "source": [
        "# Random embedding evaluation with trainable=False\n",
        "model.evaluate(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC_7yaQU2vxA"
      },
      "source": [
        "# Random initialisation with sigmoid outputs\n",
        "model.evaluate(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yQbxDt6rprg"
      },
      "source": [
        "model.evaluate(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Lt0I-zsZPe"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYLp2YyL4vuN"
      },
      "source": [
        "# model.evaluate(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea-MzBc5uFjf"
      },
      "source": [
        "# model.evaluate(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqX9sKXiqtVl"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yhhxSobtMVV"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_F_37KR410M"
      },
      "source": [
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLw9tBU3853d"
      },
      "source": [
        "len(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PbYboCRnBEL"
      },
      "source": [
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxy98mM8--Nq"
      },
      "source": [
        "#Spacey fold-2\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WmQ1x88pJCp"
      },
      "source": [
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULrYhatB_3pL"
      },
      "source": [
        "# Random fold-2\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxMpgHk22X1L"
      },
      "source": [
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NekZyy5b9a9W"
      },
      "source": [
        "plt.plot(history.history['mean_absolute_percentage_error'], label='Train')\n",
        "plt.plot(history.history['val_mean_absolute_percentage_error'], label='Validation')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbDYnInZ9Z3B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IpAxG3V9G6m"
      },
      "source": [
        "model.save_weights('audio_embedding.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHgSk3352XL6"
      },
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/E3/model_5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yfFFxfrpw0S"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ra7Nhpl3Jdn"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGB0Lfcz5DbH"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j7LyLJT5HKy"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APA63D3-q-Vc"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0kxARxI62OP"
      },
      "source": [
        "# random\n",
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDYxBSdd7MXv"
      },
      "source": [
        "# +3 db\n",
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB9AKkUmGOsd"
      },
      "source": [
        "# +3 db\n",
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LHu9msDQfqX"
      },
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/E2/model_3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THyH2PBg00zL"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/audio-and-word-embeddings/E2/model_3_backup.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsDx-wK7SRWa"
      },
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/E2/model_3_backup_weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-UgLn6DVUXy"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/audio-and-word-embeddings/E2/model_3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-jg1BIE9X1X"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpXlWcUJQyT"
      },
      "source": [
        "test_words = [\"metallic\"]\n",
        "test_words_vec = vec2(np.array([[s] for s in test_words])).numpy()\n",
        "p = model.predict(test_words_vec)\n",
        "# y_test = (y_test - mmin) / (mmax - mmin)\n",
        "p = p * (mmax - mmin) + mmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Re38daLQNs"
      },
      "source": [
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJAYUXC_MYdO"
      },
      "source": [
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seYAKUCANxps"
      },
      "source": [
        "p2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ26P4S1SbXR"
      },
      "source": [
        "!pip install git+https://github.com/detly/gammatone.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWnPCP03WPQw"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FV5KDNVsBpb1fRp2kmyWxDhL1EfvvqAP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1FV5KDNVsBpb1fRp2kmyWxDhL1EfvvqAP\" -O signals.zip && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akA-VY9JbbVw"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FV5KDNVsBpb1fRp2kmyWxDhL1EfvvqAP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1FV5KDNVsBpb1fRp2kmyWxDhL1EfvvqAP\" -O signals.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip signals.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aILqgl1kWRMc"
      },
      "source": [
        "from gammatone import filters \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import IPython.display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnFN7UDIWWHn"
      },
      "source": [
        "# Load and listen to original audio sample\n",
        "\n",
        "x, fs = sf.read('signals/e_gtr_short.wav')\n",
        "print(f\"Loaded {x.shape[0]} samples at fs={fs}\")\n",
        "IPython.display.Audio(x,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOGbO0JzWXbC"
      },
      "source": [
        "# A list of terms taken from the SocialFX Dataset\n",
        "gain_vector = {}\n",
        "gain_vector['hot'] = np.flip(np.array([0.797209, -1.287298455, -1.532964082, -1.506539385, -1.396973938, -1.271198913, -1.017866669, -0.528729007, 0.165573584, 0.754400175, 1.073992036, 1.282210725, 1.442060062, 1.568685481, 1.653999697, 1.648503806, 1.53967359, 1.580347112, 1.577686577, 1.205675471, 0.676758791, 0.252821123, 0.075167078, -0.033101099, -0.089115047, -0.1365393, -0.143565939, -0.084299287, -0.051027767, -0.038591153, -0.089749952, -0.192244295, -0.345006204, -0.529255497, -0.724989266, -0.864740603, -0.916542181, -0.839557577, -0.823611993, -0.980958406]))\n",
        "gain_vector['warm'] = np.flip(np.array([0.806225, 1.970152522, 2.063643883, 2.109621503, 2.019063089, 1.833557153, 1.54050257, 1.253791039, 1.124354285, 0.912120468, 0.479917036, -0.176767683, -0.766548101, -0.982288412, -0.876476603, -0.871483715, -0.986591984, -1.095373269, -1.006018325, -0.665307271, -0.386414479, -0.282319476, -0.236480253, -0.182852135, -0.094328345, 0.115664993, 0.429528178, 0.443207819, 0.225592816, -0.061474359, -0.435409903, -0.866104454, -1.093506922, -1.048872922, -0.838433617, -0.650190554, -0.567333094, -0.666182754, -0.760821358, -0.622340305]))\n",
        "gain_vector['cold'] = np.flip(np.array([0.56672, -2.034095236, -2.054271815, -2.050160073, -1.971859231, -1.880512554, -1.774836615, -1.588498521, -1.323855231, -0.929419044, -0.477990841, 0.001836709, 0.425480213, 0.710446169, 0.736694949, 0.743315275, 0.699904753, 0.573891718, 0.417793993, 0.336596002, 0.362311733, 0.393507005, 0.31382609, 0.206445573, 0.156956594, 0.131116367, 0.096204939, 0.101053811, 0.198440373, 0.395838345, 0.665566916, 0.984723167, 1.146000944, 0.996436067, 0.733647623, 0.466518535, 0.551724202, 0.883208881, 0.863160135, 0.817721258]))\n",
        "gain_vector['relaxing'] = np.flip(np.array([0.667283, 2.12661933, 1.972813587, 2.076840264, 2.029614099, 1.828261577, 1.529357681, 1.106721532, 0.574096502, 0.036519114, -0.369146913, -0.763306351, -1.214185152, -1.591751218, -1.730069773, -1.572405592, -1.283399138, -1.112799795, -0.831637822, -0.283981666, 0.018265935, 0.070173219, -0.021844884, -0.107969439, -0.090211942, 0.099998408, 0.417992371, 0.497223147, 0.322845287, 0.05663085, -0.207602229, -0.375132475, -0.362059187, -0.237172682, -0.145072176, -0.271109014, -0.57736182, -0.58486884, -0.355700261, -0.28738431]))\n",
        "gain_vector['soothing'] = np.flip(np.array([0.668623, -0.342018362, 0.026666401, 0.446729242, 0.870928285, 1.273066751, 1.605112245, 1.788250756, 1.713096735, 1.410331198, 0.944353776, 0.307533749, -0.378830743, -1.014190533, -1.536209856, -1.920257823, -2.106073864, -2.078396015, -1.942651727, -1.623318874, -1.065090653, -0.478780769, 0.026683737, 0.352843499, 0.58902601, 0.684407324, 0.492285815, 0.224337514, 0.199443126, 0.277549186, 0.356415253, 0.388200774, 0.289154264, 0.096006927, 0.001571427, 0.070259291, 0.231924942, 0.189760673, -0.05734363, -0.202312582]))\n",
        "gain_vector['harsh'] = np.flip(np.array([0.444456, -0.763625387, -1.097657151, -1.109271826, -0.828296799, -0.504376203, -0.175019514, 0.112461643, 0.343172372, 0.48092289, 0.578483124, 0.793573687, 1.1619776, 1.550687911, 1.789115385, 1.813865965, 1.528659423, 1.337018759, 1.303155904, 1.032093521, 0.634841813, 0.406146777, 0.415351652, 0.425582285, 0.305761841, 0.050915992, -0.345361682, -0.840566551, -1.239371225, -1.504682027, -1.659385274, -1.677605366, -1.570754566, -1.349434626, -1.050155243, -0.736486167, -0.341679134, -0.02872275, 0.067719463, 0.195935989]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUh-AxIkhPkG"
      },
      "source": [
        "\"cold\" in train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omufyu5JWeYJ"
      },
      "source": [
        "# Create filters\n",
        "# - I have checked the center frequencies and implementation of the filters, and they all match up the same from the paper.\n",
        "c_freqs = filters.centre_freqs(fs, 40, 20)\n",
        "fcoefs = filters.make_erb_filters(fs, c_freqs, width=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4vD4b3F-zV0"
      },
      "source": [
        "gain_vector['hot'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFIxk12w-uFt"
      },
      "source": [
        "c_freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB3jK5c6WLgm"
      },
      "source": [
        "test_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LM7CX9CZQ1-"
      },
      "source": [
        "val_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLcLOODRasjc"
      },
      "source": [
        "train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egz-zzl9kXMp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf6HBDXIkXTH"
      },
      "source": [
        "model.load_weights(\"/content/spacey-2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphu1sGDZU-s"
      },
      "source": [
        "\"harsh\" in train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbKAGwAPWgrm"
      },
      "source": [
        "# Apply filterbank to audio signal and apply a given gain waiting value  to each band.\n",
        "# Select a term (from the list above) and listen.\n",
        "filterbank_x = filters.erb_filterbank(x, fcoefs)\n",
        "out = np.sum(filterbank_x.T * gain_vector['metal'],axis=1)\n",
        "IPython.display.Audio(out,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d_g6GwbkghC"
      },
      "source": [
        "\"hot\" in train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XwlWmcPu9c8"
      },
      "source": [
        "len(val_word_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tyTTKeBX8jK"
      },
      "source": [
        "test_words = [\"harsh\"]\n",
        "test_words_vec = vec2(np.array([[s] for s in test_words])).numpy()\n",
        "p = model.predict(test_words_vec)\n",
        "# y_test = (y_test - mmin) / (mmax - mmin)\n",
        "p_harsh = p * (mmax - mmin) + mmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5XfplHCjrB9"
      },
      "source": [
        "mmax - mmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5oYMk9GjpGR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxIdVLwPjpKb"
      },
      "source": [
        "p_harsh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Xc77x3TT6d"
      },
      "source": [
        "test_words_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca3UoQKKWiBL"
      },
      "source": [
        "# Apply filterbank to audio signal and apply a given gain waiting value  to each band.\n",
        "# Select a term (from the list above) and listen.\n",
        "filterbank_x = filters.erb_filterbank(x, fcoefs)\n",
        "out = np.sum(filterbank_x.T * np.flip(p[0, :]),axis=1)\n",
        "IPython.display.Audio(out,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-LCouX6k0hX"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAP2QtqWk9zC"
      },
      "source": [
        "p_sharp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWhpC-geL9Xc"
      },
      "source": [
        "plt.plot(np.flip(gain_vector['hot']), label=\"hot\")\n",
        "plt.plot(p_hot[0, :], label=\"pred_hot\")\n",
        "plt.plot(p_hot_2[0, :], label=\"pred_hot_random\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HApC3v5QoFgE"
      },
      "source": [
        "plt.plot(np.flip(gain_vector['harsh']), label=\"harsh\")\n",
        "plt.plot(p_harsh[0, :], label=\"pred_harsh\")\n",
        "plt.plot(p_harsh_2[0, :], label=\"pred_harsh_random\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJndwCQbt1Rv"
      },
      "source": [
        "abs = tf.keras.losses.MeanAbsoluteError(\n",
        "    reduction=\"auto\", name=\"mean_absolute_error\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJgcMMHGt2oY"
      },
      "source": [
        "abs(np.flip(gain_vector['harsh']), p_harsh[0, :]).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0dJEGBjuBAe"
      },
      "source": [
        "abs(np.flip(gain_vector['harsh']), p_harsh_2[0, :]).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDWb6lHbYJdM"
      },
      "source": [
        "plt.plot(p_bright[0, :], label=\"bright\")\n",
        "plt.plot(p_warm[0, :], label=\"warm\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyGnuSCtYzfq"
      },
      "source": [
        "plt.plot(p_harsh[0, :], label=\"harsh\")\n",
        "plt.plot(p_warm[0, :], label=\"warm\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_zVCKnjag4A"
      },
      "source": [
        "plt.plot(p_metal[0, :], label=\"metal\")\n",
        "plt.plot(p_warm[0, :], label=\"warm\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTyw70QoZrK7"
      },
      "source": [
        "# Apply filterbank to audio signal and apply a given gain waiting value  to each band.\n",
        "# Select a term (from the list above) and listen.\n",
        "filterbank_x = filters.erb_filterbank(x, fcoefs)\n",
        "out = np.sum(filterbank_x.T * p2[0, :-1],axis=1)\n",
        "IPython.display.Audio(out,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67N6Vsl7YVQr"
      },
      "source": [
        "\"sharp\" in train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTMS0Fe5Wyrc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBHOBdkpNsic"
      },
      "source": [
        "# Testing with audio files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp6pR_f5NthB"
      },
      "source": [
        "# Load and listen to original audio sample\n",
        "\n",
        "x, fs = sf.read('signals/e_gtr_short.wav')\n",
        "print(f\"Loaded {x.shape[0]} samples at fs={fs}\")\n",
        "IPython.display.Audio(x,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJt5O-iaNuiv"
      },
      "source": [
        "test_words = [\"harsh\"]\n",
        "test_words_vec = vec2(np.array([[s] for s in test_words])).numpy()\n",
        "p = model.predict(test_words_vec)\n",
        "# y_test = (y_test - mmin) / (mmax - mmin)\n",
        "p = p * (mmax - mmin) + mmin\n",
        "\n",
        "# Apply filterbank to audio signal and apply a given gain waiting value  to each band.\n",
        "# Select a term (from the list above) and listen.\n",
        "filterbank_x = filters.erb_filterbank(x, fcoefs)\n",
        "out = np.sum(filterbank_x.T * np.flip(p[0, :]),axis=1)\n",
        "IPython.display.Audio(out,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUfT0j9MPiWp"
      },
      "source": [
        "test_words = [\"cold\"]\n",
        "test_words_vec = vec2(np.array([[s] for s in test_words])).numpy()\n",
        "p2 = model.predict(test_words_vec)\n",
        "# y_test = (y_test - mmin) / (mmax - mmin)\n",
        "p2 = p2 * (mmax - mmin) + mmin\n",
        "\n",
        "# Apply filterbank to audio signal and apply a given gain waiting value  to each band.\n",
        "# Select a term (from the list above) and listen.\n",
        "filterbank_x = filters.erb_filterbank(x, fcoefs)\n",
        "out = np.sum(filterbank_x.T * np.flip(p2[0, :-1]),axis=1)\n",
        "IPython.display.Audio(out,rate=fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKmZuheVPtOM"
      },
      "source": [
        "p == p2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGnulG_gN2Hg"
      },
      "source": [
        "\"loud\" in train_word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11dFtsCNPsHv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FyhxSwDN5U5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjz-gaG--sic"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
